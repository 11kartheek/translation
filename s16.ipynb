{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30733,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-06-10T08:36:44.135716Z","iopub.execute_input":"2024-06-10T08:36:44.136406Z","iopub.status.idle":"2024-06-10T08:36:45.349947Z","shell.execute_reply.started":"2024-06-10T08:36:44.136377Z","shell.execute_reply":"2024-06-10T08:36:45.349134Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"!pip install tokenizers\n!pip install torchtext\n!pip install pytorch_lightning\n!pip install datasets\n!pip install tensorboard","metadata":{"execution":{"iopub.status.busy":"2024-06-10T08:37:06.689200Z","iopub.execute_input":"2024-06-10T08:37:06.690092Z","iopub.status.idle":"2024-06-10T08:38:10.431750Z","shell.execute_reply.started":"2024-06-10T08:37:06.690062Z","shell.execute_reply":"2024-06-10T08:38:10.430409Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Requirement already satisfied: tokenizers in /opt/conda/lib/python3.10/site-packages (0.19.1)\nRequirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /opt/conda/lib/python3.10/site-packages (from tokenizers) (0.23.2)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers) (3.13.1)\nRequirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers) (2024.3.1)\nRequirement already satisfied: packaging>=20.9 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers) (6.0.1)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers) (2.32.3)\nRequirement already satisfied: tqdm>=4.42.1 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers) (4.66.4)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers) (4.9.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.9->huggingface-hub<1.0,>=0.16.4->tokenizers) (3.1.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub<1.0,>=0.16.4->tokenizers) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub<1.0,>=0.16.4->tokenizers) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub<1.0,>=0.16.4->tokenizers) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub<1.0,>=0.16.4->tokenizers) (2024.2.2)\nRequirement already satisfied: torchtext in /opt/conda/lib/python3.10/site-packages (0.16.2)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from torchtext) (4.66.4)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from torchtext) (2.32.3)\nRequirement already satisfied: torch in /opt/conda/lib/python3.10/site-packages (from torchtext) (2.1.2)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from torchtext) (1.26.4)\nRequirement already satisfied: torchdata in /opt/conda/lib/python3.10/site-packages (from torchtext) (0.7.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->torchtext) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->torchtext) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->torchtext) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->torchtext) (2024.2.2)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch->torchtext) (3.13.1)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch->torchtext) (4.9.0)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch->torchtext) (1.12.1)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch->torchtext) (3.2.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch->torchtext) (3.1.2)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch->torchtext) (2024.3.1)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch->torchtext) (2.1.3)\nRequirement already satisfied: mpmath<1.4.0,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch->torchtext) (1.3.0)\nRequirement already satisfied: pytorch_lightning in /opt/conda/lib/python3.10/site-packages (2.2.5)\nRequirement already satisfied: numpy>=1.17.2 in /opt/conda/lib/python3.10/site-packages (from pytorch_lightning) (1.26.4)\nRequirement already satisfied: torch>=1.13.0 in /opt/conda/lib/python3.10/site-packages (from pytorch_lightning) (2.1.2)\nRequirement already satisfied: tqdm>=4.57.0 in /opt/conda/lib/python3.10/site-packages (from pytorch_lightning) (4.66.4)\nRequirement already satisfied: PyYAML>=5.4 in /opt/conda/lib/python3.10/site-packages (from pytorch_lightning) (6.0.1)\nRequirement already satisfied: fsspec>=2022.5.0 in /opt/conda/lib/python3.10/site-packages (from fsspec[http]>=2022.5.0->pytorch_lightning) (2024.3.1)\nRequirement already satisfied: torchmetrics>=0.7.0 in /opt/conda/lib/python3.10/site-packages (from pytorch_lightning) (1.4.0.post0)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from pytorch_lightning) (21.3)\nRequirement already satisfied: typing-extensions>=4.4.0 in /opt/conda/lib/python3.10/site-packages (from pytorch_lightning) (4.9.0)\nRequirement already satisfied: lightning-utilities>=0.8.0 in /opt/conda/lib/python3.10/site-packages (from pytorch_lightning) (0.11.2)\nRequirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /opt/conda/lib/python3.10/site-packages (from fsspec[http]>=2022.5.0->pytorch_lightning) (3.9.1)\nRequirement already satisfied: setuptools in /opt/conda/lib/python3.10/site-packages (from lightning-utilities>=0.8.0->pytorch_lightning) (69.0.3)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->pytorch_lightning) (3.1.1)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->pytorch_lightning) (3.13.1)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->pytorch_lightning) (1.12.1)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->pytorch_lightning) (3.2.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->pytorch_lightning) (3.1.2)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch_lightning) (23.2.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch_lightning) (6.0.4)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch_lightning) (1.9.3)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch_lightning) (1.4.1)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch_lightning) (1.3.1)\nRequirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch_lightning) (4.0.3)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.13.0->pytorch_lightning) (2.1.3)\nRequirement already satisfied: mpmath<1.4.0,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.13.0->pytorch_lightning) (1.3.0)\nRequirement already satisfied: idna>=2.0 in /opt/conda/lib/python3.10/site-packages (from yarl<2.0,>=1.0->aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch_lightning) (3.6)\nRequirement already satisfied: datasets in /opt/conda/lib/python3.10/site-packages (2.19.2)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from datasets) (3.13.1)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from datasets) (1.26.4)\nRequirement already satisfied: pyarrow>=12.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (14.0.2)\nRequirement already satisfied: pyarrow-hotfix in /opt/conda/lib/python3.10/site-packages (from datasets) (0.6)\nRequirement already satisfied: dill<0.3.9,>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (0.3.8)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from datasets) (2.2.1)\nRequirement already satisfied: requests>=2.32.1 in /opt/conda/lib/python3.10/site-packages (from datasets) (2.32.3)\nRequirement already satisfied: tqdm>=4.62.1 in /opt/conda/lib/python3.10/site-packages (from datasets) (4.66.4)\nRequirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from datasets) (3.4.1)\nRequirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from datasets) (0.70.16)\nRequirement already satisfied: fsspec<=2024.3.1,>=2023.1.0 in /opt/conda/lib/python3.10/site-packages (from fsspec[http]<=2024.3.1,>=2023.1.0->datasets) (2024.3.1)\nRequirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets) (3.9.1)\nRequirement already satisfied: huggingface-hub>=0.21.2 in /opt/conda/lib/python3.10/site-packages (from datasets) (0.23.2)\nRequirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from datasets) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from datasets) (6.0.1)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (23.2.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (6.0.4)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.9.3)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.4.1)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.3.1)\nRequirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (4.0.3)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.21.2->datasets) (4.9.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging->datasets) (3.1.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests>=2.32.1->datasets) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.32.1->datasets) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.32.1->datasets) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.32.1->datasets) (2024.2.2)\nRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2023.3.post1)\nRequirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2023.4)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\nRequirement already satisfied: tensorboard in /opt/conda/lib/python3.10/site-packages (2.15.1)\nRequirement already satisfied: absl-py>=0.4 in /opt/conda/lib/python3.10/site-packages (from tensorboard) (1.4.0)\nRequirement already satisfied: grpcio>=1.48.2 in /opt/conda/lib/python3.10/site-packages (from tensorboard) (1.59.3)\nRequirement already satisfied: google-auth<3,>=1.6.3 in /opt/conda/lib/python3.10/site-packages (from tensorboard) (2.26.1)\nRequirement already satisfied: google-auth-oauthlib<2,>=0.5 in /opt/conda/lib/python3.10/site-packages (from tensorboard) (1.2.0)\nRequirement already satisfied: markdown>=2.6.8 in /opt/conda/lib/python3.10/site-packages (from tensorboard) (3.5.2)\nRequirement already satisfied: numpy>=1.12.0 in /opt/conda/lib/python3.10/site-packages (from tensorboard) (1.26.4)\nRequirement already satisfied: protobuf<4.24,>=3.19.6 in /opt/conda/lib/python3.10/site-packages (from tensorboard) (3.20.3)\nRequirement already satisfied: requests<3,>=2.21.0 in /opt/conda/lib/python3.10/site-packages (from tensorboard) (2.32.3)\nRequirement already satisfied: setuptools>=41.0.0 in /opt/conda/lib/python3.10/site-packages (from tensorboard) (69.0.3)\nRequirement already satisfied: six>1.9 in /opt/conda/lib/python3.10/site-packages (from tensorboard) (1.16.0)\nRequirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /opt/conda/lib/python3.10/site-packages (from tensorboard) (0.7.2)\nRequirement already satisfied: werkzeug>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from tensorboard) (3.0.3)\nRequirement already satisfied: cachetools<6.0,>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard) (4.2.4)\nRequirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard) (0.3.0)\nRequirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard) (4.9)\nRequirement already satisfied: requests-oauthlib>=0.7.0 in /opt/conda/lib/python3.10/site-packages (from google-auth-oauthlib<2,>=0.5->tensorboard) (1.3.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard) (2024.2.2)\nRequirement already satisfied: MarkupSafe>=2.1.1 in /opt/conda/lib/python3.10/site-packages (from werkzeug>=1.0.1->tensorboard) (2.1.3)\nRequirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /opt/conda/lib/python3.10/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard) (0.5.1)\nRequirement already satisfied: oauthlib>=3.0.0 in /opt/conda/lib/python3.10/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard) (3.2.2)\n","output_type":"stream"}]},{"cell_type":"code","source":"!git clone https://github.com/11kartheek/translation.git","metadata":{"execution":{"iopub.status.busy":"2024-06-10T08:38:10.433923Z","iopub.execute_input":"2024-06-10T08:38:10.434234Z","iopub.status.idle":"2024-06-10T08:38:12.006393Z","shell.execute_reply.started":"2024-06-10T08:38:10.434205Z","shell.execute_reply":"2024-06-10T08:38:12.005177Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"Cloning into 'translation'...\nremote: Enumerating objects: 38, done.\u001b[K\nremote: Counting objects: 100% (38/38), done.\u001b[K\nremote: Compressing objects: 100% (35/35), done.\u001b[K\nremote: Total 38 (delta 15), reused 0 (delta 0), pack-reused 0\u001b[K\nUnpacking objects: 100% (38/38), 32.24 KiB | 1.54 MiB/s, done.\n","output_type":"stream"}]},{"cell_type":"code","source":"%cd translation","metadata":{"execution":{"iopub.status.busy":"2024-06-10T08:39:03.528386Z","iopub.execute_input":"2024-06-10T08:39:03.529350Z","iopub.status.idle":"2024-06-10T08:39:03.535752Z","shell.execute_reply.started":"2024-06-10T08:39:03.529302Z","shell.execute_reply":"2024-06-10T08:39:03.534889Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"/kaggle/working/translation\n","output_type":"stream"}]},{"cell_type":"code","source":"from config_file import get_config\n","metadata":{"execution":{"iopub.status.busy":"2024-06-10T08:39:07.082738Z","iopub.execute_input":"2024-06-10T08:39:07.083573Z","iopub.status.idle":"2024-06-10T08:39:07.089499Z","shell.execute_reply.started":"2024-06-10T08:39:07.083542Z","shell.execute_reply":"2024-06-10T08:39:07.088508Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"from model import build_transformer\nfrom dataset import BillingualDataset, casual_mask\nfrom config_file import get_config, get_weights_file_path\n\nimport torchtext.datasets as datasets\nimport torch\ntorch.cuda.amp.autocast(enabled = True)\n\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader, random_split\nfrom torch.optim.lr_scheduler import LambdaLR\n\nimport warnings\nfrom tqdm import tqdm\nimport os\nfrom pathlib import Path\n\nfrom datasets import load_dataset\nfrom tokenizers import Tokenizer\nfrom tokenizers.models import WordLevel\nfrom tokenizers.trainers import WordLevelTrainer\nfrom tokenizers.pre_tokenizers import Whitespace\n\nimport torchmetrics\nfrom torch.utils.tensorboard import SummaryWriter\n\ntorch.cuda.empty_cache()\nos.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"max_split_size_mb:12240\"\nconfig = get_config()\n\nfrom typing import Tuple, Optional, Callable\n\nimport torch\nfrom torch.optim.optimizer import Optimizer\n\n","metadata":{"execution":{"iopub.status.busy":"2024-06-10T08:39:08.093269Z","iopub.execute_input":"2024-06-10T08:39:08.093643Z","iopub.status.idle":"2024-06-10T08:39:29.985755Z","shell.execute_reply.started":"2024-06-10T08:39:08.093614Z","shell.execute_reply":"2024-06-10T08:39:29.984857Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stderr","text":"2024-06-10 08:39:19.563199: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-06-10 08:39:19.563312: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-06-10 08:39:19.720308: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}]},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader, random_split\nfrom torch.nn.utils.rnn import pad_sequence\nfrom datasets import load_dataset\n\nclass BillingualDataset(Dataset):\n    def __init__(self, ds, tokenizer_src, tokenizer_tgt, src_lang, tgt_lang):\n        super().__init__()\n        self.ds = ds  # The dataset containing the parallel corpora\n        self.tokenizer_src = tokenizer_src  # Tokenizer for the source language\n        self.tokenizer_tgt = tokenizer_tgt  # Tokenizer for the target language\n        self.src_lang = src_lang  # Source language code\n        self.tgt_lang = tgt_lang  # Target language code\n        \n        # Tokens for start-of-sequence, end-of-sequence, and padding\n        self.sos_token = torch.tensor([tokenizer_tgt.token_to_id(\"[SOS]\")], dtype=torch.int64)\n        self.eos_token = torch.tensor([tokenizer_tgt.token_to_id(\"[EOS]\")], dtype=torch.int64)\n        self.pad_token = torch.tensor([tokenizer_tgt.token_to_id(\"[PAD]\")], dtype=torch.int64)\n        \n    def __len__(self):\n        return len(self.ds)\n    \n    def __getitem__(self, idx):\n        src_tgt_pair = self.ds[idx]\n        src_text = src_tgt_pair['translation'][self.src_lang]\n        tgt_text = src_tgt_pair['translation'][self.tgt_lang]\n        \n        # Tokenize the source and target texts\n        enc_input_tokens = self.tokenizer_src.encode(src_text).ids\n        dec_input_tokens = self.tokenizer_tgt.encode(tgt_text).ids\n        \n        # Create the encoder input without padding\n        encoder_input = torch.tensor(\n            [self.sos_token.item()] + enc_input_tokens + [self.eos_token.item()],\n            dtype=torch.int64\n        )\n        \n        # Create the decoder input without padding\n        decoder_input = torch.tensor(\n            [self.sos_token.item()] + dec_input_tokens,\n            dtype=torch.int64\n        )\n        \n        # Create the label without padding\n        label = torch.tensor(\n            dec_input_tokens + [self.eos_token.item()],\n            dtype=torch.int64\n        )\n        \n        return {\n            \"encoder_input\": encoder_input,\n            \"decoder_input\": decoder_input,\n            \"label\": label,\n            \"src_text\": src_text,\n            \"tgt_text\": tgt_text\n        }\n\ndef smart_batching(dataset, batch_size):\n    sorted_data = sorted(dataset, key=lambda x: len(x['encoder_input']))  # Sort by source sequence length\n    batches = [sorted_data[i:i + batch_size] for i in range(0, len(sorted_data), batch_size)]  # Create batches\n    return batches\n\ndef collate_fn(batch):\n    encoder_inputs = [item['encoder_input'] for item in batch]\n    decoder_inputs = [item['decoder_input'] for item in batch]\n    labels = [item['label'] for item in batch]\n    \n    encoder_inputs_padded = pad_sequence(encoder_inputs, batch_first=True, padding_value=0)  # Pad encoder inputs\n    decoder_inputs_padded = pad_sequence(decoder_inputs, batch_first=True, padding_value=0)  # Pad decoder inputs\n    labels_padded = pad_sequence(labels, batch_first=True, padding_value=0)  # Pad labels\n    \n    # Expand dimensions for masks\n    encoder_mask = (encoder_inputs_padded != 0).unsqueeze(1).unsqueeze(1).int()  # Create encoder mask\n    decoder_mask = (decoder_inputs_padded != 0).unsqueeze(1).unsqueeze(1).int() & causal_mask(decoder_inputs_padded.size(1))  # Create decoder mask\n    \n    return {\n        \"encoder_input\": encoder_inputs_padded,\n        \"decoder_input\": decoder_inputs_padded,\n        \"encoder_mask\": encoder_mask,\n        \"decoder_mask\": decoder_mask,\n        \"label\": labels_padded,\n    }\n\n\ndef causal_mask(size):\n    mask = torch.triu(torch.ones((size, size)), diagonal=1).type(torch.int)  # Upper triangular mask\n    return mask == 0  # Convert to boolean mask\n\ndef get_ds(config):\n    ds_raw = load_dataset('opus_books', f\"{config['lang_src']}-{config['lang_tgt']}\", split='train')\n    \n    src_lang = config[\"lang_src\"]\n    tgt_lang = config[\"lang_tgt\"]\n    \n    tokenizer_src = get_or_build_tokenizer(config, ds_raw, src_lang)\n    tokenizer_tgt = get_or_build_tokenizer(config, ds_raw, tgt_lang)\n    \n    train_ds_size = int(0.9 * len(ds_raw))\n    val_ds_size = len(ds_raw) - train_ds_size\n    train_ds_raw, val_ds_raw = random_split(ds_raw, [train_ds_size, val_ds_size])\n    \n    train_ds = BillingualDataset(train_ds_raw, tokenizer_src, tokenizer_tgt, src_lang, tgt_lang)\n    val_ds = BillingualDataset(val_ds_raw, tokenizer_src, tokenizer_tgt, src_lang, tgt_lang)\n    \n    max_len_src = max(len(tokenizer_src.encode(item['translation'][src_lang]).ids) for item in ds_raw)\n    max_len_tgt = max(len(tokenizer_tgt.encode(item['translation'][tgt_lang]).ids) for item in ds_raw)\n    \n    print(f\"Max length of the source sentence: {max_len_src}\")\n    print(f\"Max length of the target sentence: {max_len_tgt}\")\n    \n    train_batches = smart_batching(train_ds, config[\"batch_size\"])  # Smart batching for training data\n    val_batches = smart_batching(val_ds, 1)  # Smart batching for validation data\n    \n    train_dataloader = DataLoader(train_batches, batch_size=None, collate_fn=collate_fn, shuffle=True)  # Train DataLoader\n    val_dataloader = DataLoader(val_batches, batch_size=None, collate_fn=collate_fn, shuffle=True)  # Validation DataLoader\n    \n    return train_dataloader, val_dataloader, tokenizer_src, tokenizer_tgt\n","metadata":{"execution":{"iopub.status.busy":"2024-06-10T08:39:29.987251Z","iopub.execute_input":"2024-06-10T08:39:29.987832Z","iopub.status.idle":"2024-06-10T08:39:30.012709Z","shell.execute_reply.started":"2024-06-10T08:39:29.987803Z","shell.execute_reply":"2024-06-10T08:39:30.011791Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"from model import build_transformer\n# from dataset import BillingualDataset, casual_mask\nfrom config_file import get_config, get_weights_file_path\n\nimport torchtext.datasets as datasets\nimport torch\ntorch.cuda.amp.autocast(enabled = True)\n\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader, random_split\nfrom torch.optim.lr_scheduler import LambdaLR\n\nimport warnings\nfrom tqdm import tqdm\nimport os\nfrom pathlib import Path\n\nfrom datasets import load_dataset\nfrom tokenizers import Tokenizer\nfrom tokenizers.models import WordLevel\nfrom tokenizers.trainers import WordLevelTrainer\nfrom tokenizers.pre_tokenizers import Whitespace\n\nimport torchmetrics\nfrom torch.utils.tensorboard import SummaryWriter\n\ntorch.cuda.empty_cache()\nos.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"max_split_size_mb:12240\"\nconfig = get_config()\n\nfrom typing import Tuple, Optional, Callable\n\nimport torch\nfrom torch.optim.optimizer import Optimizer\n\ndef exists(val):\n    return val is not None\n\n# update functions\n\ndef update_fn(p, grad, exp_avg, lr, wd, beta1, beta2):\n    # stepweight decay\n\n    p.data.mul_(1 - lr * wd)\n\n    # weight update\n\n    update = exp_avg.clone().mul_(beta1).add(grad, alpha = 1 - beta1).sign_()\n    p.add_(update, alpha = -lr)\n\n    # decay the momentum running average coefficient\n\n    exp_avg.mul_(beta2).add_(grad, alpha = 1 - beta2)\n\n# class\n\nclass Lion(Optimizer):\n    def __init__(\n        self,\n        params,\n        lr: float = 1e-4,\n        betas: Tuple[float, float] = (0.9, 0.99),\n        weight_decay: float = 0.0\n    ):\n        assert lr > 0.\n        assert all([0. <= beta <= 1. for beta in betas])\n\n        defaults = dict(\n            lr = lr,\n            betas = betas,\n            weight_decay = weight_decay\n        )\n\n        super().__init__(params, defaults)\n\n        self.update_fn = update_fn\n\n        \n\n    @torch.no_grad()\n    def step(\n        self,\n        closure: Optional[Callable] = None\n    ):\n\n        loss = None\n        if exists(closure):\n            with torch.enable_grad():\n                loss = closure()\n\n        for group in self.param_groups:\n            for p in filter(lambda p: exists(p.grad), group['params']):\n\n                grad, lr, wd, beta1, beta2, state = p.grad, group['lr'], group['weight_decay'], *group['betas'], self.state[p]\n\n                # init state - exponential moving average of gradient values\n\n                if len(state) == 0:\n                    state['exp_avg'] = torch.zeros_like(p)\n\n                exp_avg = state['exp_avg']\n\n                self.update_fn(\n                    p,\n                    grad,\n                    exp_avg,\n                    lr,\n                    wd,\n                    beta1,\n                    beta2\n                )\n\n        return loss\n\ndef greedy_decode(model, source, source_mask, tokenizer_src, tokenizer_tgt, max_len, device):\n    \n    \n    sos_idx = tokenizer_tgt.token_to_id('[SOS]')\n    eos_idx = tokenizer_tgt.token_to_id('[EOS]')\n    \n    encoder_output = model.encode(source, source_mask)\n    #Initialize the decoder input with SOS token\n    decoder_input = torch.empty(1, 1).fill_(sos_idx).type_as(source).to(device)\n    while True:\n        if decoder_input.size(1) == max_len:\n            break\n        decoder_mask = casual_mask(decoder_input.size(1)).type_as(source_mask).to(device)\n        out = model.decode(encoder_output, source_mask, decoder_input, decoder_mask)\n        \n        prob = model.project(out[:, -1])\n        _, next_word = torch.max(prob, dim=1)\n        decoder_input = torch.cat(\n            [\n                decoder_input,\n                torch.empty(1, 1).type_as(source_mask).fill_(next_word.item()).to(device)\n            ],\n            dim =  1\n        )\n        \n        if next_word == eos_idx:\n            break\n        \n    return decoder_input.squeeze(0)\n\n\ndef run_validation(model, val_dataloader, tokenizer_src, tokenizer_tgt, max_len, device, writer, global_step):\n    model.eval()\n    count = 0\n    source_texts = []\n    expected = []\n    predicted = []\n    \n    try:\n        with os.popen('stty size', 'r') as console:\n            _, console_width = console.read().split()\n            console_width = int(console_width)\n    except:\n        console_width = 80\n        \n    with torch.no_grad():\n        for batch in val_dataloader:\n            count += 1\n            encoder_input = batch[\"encoder_input\"].to(device)\n            encoder_mask = batch[\"encoder_mask\"].to(device)\n            \n            assert encoder_input.size(0)==1, \"Batch size must be 1 for validation\"\n            model_out = greedy_decode(model, encoder_input, encoder_mask, tokenizer_src, tokenizer_tgt, max_len, device)\n            \n            source_text = batch[\"src_text\"][0]\n            target_text = batch[\"tgt_text\"][0]\n            model_out_text = tokenizer_tgt.decode(model_out.detach().cpu().numpy())\n            \n            source_texts.append(source_text)\n            expected.append(target_text)\n            predicted.append(model_out_text)\n    \"\"\"        \n            print(\"SOURCE\", source_text)\n            print(\"TARGET\", target_text)\n            print(\"PREDICTED\", model_out_text)\n            \n    if writer:\n        metric = torchmetrics.CharErrorRate()\n        cer = metric(predicted, expected)\n        writer.add_scalar('validation cer', cer, global_step)\n        writer.flush()\n        \n        metric = torchmetrics.WordErrorRate()\n        wer = metric(predicted, expected)\n        writer.add_scalar('validation wer', wer, global_step)\n        writer.flush()\n        \n        metric = torchmetrics.BLEUScore()\n        bleu = metric(predicted, expected)\n        writer.add_scalar('validation BLEU', bleu, global_step)\n        writer.flush()\n        \n     \"\"\"   \n\ndef get_all_sentenses(ds, lang):\n    for item in ds:\n        yield item['translation'][lang]\n        \ndef get_or_build_tokenizer(config, ds, lang):\n    tokenizer_path = Path(config[\"tokenizer_file\"].format(lang))\n    if not Path.exists(tokenizer_path):\n        tokenizer = Tokenizer(WordLevel(unk_token = \"[UNK]\"))\n        tokenizer.pre_tokenizer = Whitespace() \n        trainer = WordLevelTrainer(special_tokens = [\"[UNK]\", \"[SOS]\", \"[EOS]\", \"[PAD]\"], min_frequency = 2)\n        tokenizer.train_from_iterator(get_all_sentenses(ds, lang), trainer = trainer)\n        tokenizer.save(str(tokenizer_path))\n    else:\n        tokenizer = Tokenizer.from_file(str(tokenizer_path))\n\n    return tokenizer\n\n\ndef get_model(config, src_vocab_size, tgt_vocab_size):\n    model = build_transformer(src_vocab_size, tgt_vocab_size, config[\"seq_len\"], config[\"seq_len\"], d_model=config['d_model'])\n    return model\n\n\ndef train_model(config):\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    print(f\"Using device : {device}\")\n    \n    Path(config[\"model_folder\"]).mkdir(parents=True, exist_ok=True)\n    train_dataloader, val_dataloader, tokenizer_src, tokenizer_tgt = get_ds(config)\n    model = get_model(config, tokenizer_src.get_vocab_size(), tokenizer_tgt.get_vocab_size()).to(device)\n    \n    #Tensorboard\n    writer = SummaryWriter(config[\"experiment_name\"])\n    \n    #Adam is used to train each feature with a different learning rate. \n    #If some feature is appearing less, adam takes care of it\n    optimizer = Lion(model.parameters(), lr = config[\"lr\"])\n    \n    initial_epoch = 0\n    global_step = 0\n    \n    if config[\"preload\"]:\n        model_filename = get_weights_file_path(config, config[\"preload\"])\n        print(\"Preloading model {model_filename}\")\n        state = torch.load(model_filename)\n        model.load_state_dict(state[\"model_state_dict\"])\n        initial_epoch = state[\"epoch\"] + 1\n        optimizer.load_state_dict(state[\"optimizer_state_dict\"])\n        global_step = state[\"global_step\"]\n        print(\"preloaded\")\n        \n    loss_fn = nn.CrossEntropyLoss(ignore_index = tokenizer_src.token_to_id(\"[PAD]\"), label_smoothing=0.1)\n    \n    for epoch in range(initial_epoch, config[\"num_epochs\"]):\n        torch.cuda.empty_cache()\n        print(epoch)\n        model.train()\n        batch_iterator = tqdm(train_dataloader, desc = f\"Processing Epoch {epoch:02d}\")\n        \n        for batch in batch_iterator:\n            encoder_input = batch[\"encoder_input\"].to(device)\n            decoder_input = batch[\"decoder_input\"].to(device)\n            encoder_mask = batch[\"encoder_mask\"].to(device)\n            decoder_mask = batch[\"decoder_mask\"].to(device)\n            \n            encoder_output = model.encode(encoder_input, encoder_mask)\n            decoder_output = model.decode(encoder_output, encoder_mask, decoder_input, decoder_mask)\n            proj_output = model.project(decoder_output)\n            \n            label = batch[\"label\"].to(device)\n            \n            #Compute loss using cross entropy\n            tgt_vocab_size = tokenizer_tgt.get_vocab_size()\n            loss = loss_fn(proj_output.view(-1, tgt_vocab_size), label.view(-1))\n            batch_iterator.set_postfix({\"loss\": f\"{loss.item():6.3f}\"})\n\n            #Log the loss\n            writer.add_scalar('train_loss', loss.item(), global_step)\n            writer.flush()\n            \n            #Backpropogate loss\n            loss.backward()\n            \n            #Update weights\n            optimizer.step()\n            optimizer.zero_grad(set_to_none=True)\n            global_step+=1\n            \n        #run_validation(model, val_dataloader, tokenizer_src, tokenizer_tgt, config['seq_len'], device, writer, global_step)\n        \n        \n        model_filename = get_weights_file_path(config, f\"{epoch:02d}\")\n        torch.save(\n            {\n                \"epoch\": epoch,\n                \"model_state_dict\": model.state_dict(),\n                \"optimizer_state_dict\": optimizer.state_dict(),\n                \"global_step\": global_step\n            },\n            model_filename\n        )\n        \n\n    \n    \n","metadata":{"execution":{"iopub.status.busy":"2024-06-10T08:39:32.765267Z","iopub.execute_input":"2024-06-10T08:39:32.765660Z","iopub.status.idle":"2024-06-10T08:39:32.807871Z","shell.execute_reply.started":"2024-06-10T08:39:32.765630Z","shell.execute_reply":"2024-06-10T08:39:32.806971Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"           \nif __name__ == \"__main__\":\n    warnings.filterwarnings(\"ignore\")\n    config = get_config()\n    config[\"batch_size\"] = 16\n    config[\"preload\"] = None\n    config[\"num_epochs\"] = 18\n    train_model(config)","metadata":{"execution":{"iopub.status.busy":"2024-06-10T08:39:48.663310Z","iopub.execute_input":"2024-06-10T08:39:48.663671Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stdout","text":"Using device : cuda\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading readme:   0%|          | 0.00/28.1k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a742f3b4773d4cd9b689c1c2db4c8541"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/5.73M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f54fb77c89e44d5898fbc11ee16a8dde"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/32332 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b4e6a8ee90e24d4b87619056f40de56c"}},"metadata":{}},{"name":"stdout","text":"Max length of the source sentence: 309\nMax length of the target sentence: 274\n0\n","output_type":"stream"},{"name":"stderr","text":"Processing Epoch 00: 100%|██████████| 1819/1819 [03:22<00:00,  8.97it/s, loss=5.425]\n","output_type":"stream"},{"name":"stdout","text":"1\n","output_type":"stream"},{"name":"stderr","text":"Processing Epoch 02: 100%|██████████| 1819/1819 [03:26<00:00,  8.81it/s, loss=4.375]\n","output_type":"stream"},{"name":"stdout","text":"3\n","output_type":"stream"},{"name":"stderr","text":"Processing Epoch 03: 100%|██████████| 1819/1819 [03:26<00:00,  8.82it/s, loss=3.790]\n","output_type":"stream"},{"name":"stdout","text":"4\n","output_type":"stream"},{"name":"stderr","text":"Processing Epoch 04:  64%|██████▍   | 1170/1819 [02:12<01:29,  7.26it/s, loss=3.828]","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}